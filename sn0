#!/usr/bin/env python3

import os
import re
import urllib.parse
import urllib.request
import sys
from html.parser import HTMLParser
from concurrent.futures import ThreadPoolExecutor
import hashlib
from pathlib import Path
import argparse

def normalize_path(path):
    # Preserve the leading slash if it exists
    leading_slash = '/' if path.startswith('/') else ''
    
    # Split the path into parts
    parts = path.split('/')
    # Initialize a stack to keep track of the directory levels
    stack = []
    for part in parts:
        if part == '.' or part == '':
            continue
        elif part == '..':
            if stack and stack[-1] != '..':
                stack.pop()
            else:
                stack.append(part)
        else:
            stack.append(part)
    # Join the parts back together, preserving the leading slash
    return leading_slash + '/'.join(stack)

class LinkParser(HTMLParser):
    def __init__(self, base_url):
        super().__init__()
        self.base_url = base_url
        self.links = []
        self.current_tag = None

    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        if tag in ['a', 'link']:
            href = self.get_attr(attrs, 'href')
            if href is not None:
                self.add_link(href)
        elif tag in ['script', 'img', 'video', 'audio']:
            src = self.get_attr(attrs, 'src')
            if src is not None:
                self.add_link(src)

    def handle_endtag(self, tag):
        self.current_tag = None

    def handle_data(self, data):
        if self.current_tag == 'script':
            # Look for URLs in inline JavaScript
            urls = re.findall(r'(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-?=%.]+', data)
            for url in urls:
                self.add_link(url)

    def add_link(self, link):
        url = urllib.parse.urljoin(self.base_url, link)
        if url not in self.links:
            self.links.append(url)

    def get_attr(self, attrs, attr_name):
        for attr in attrs:
            if attr[0].lower() == attr_name:
                return attr[1]

def download_website(source, dest, bank, melt):
    # Download sn0.txt
    checksum_url = urllib.parse.urljoin(source, 'sn0.txt')
    try:
        with urllib.request.urlopen(checksum_url) as response:
            checksum_file = response.read()
        with open(Path(dest) / 'sn0.txt', 'wb') as f:
            f.write(checksum_file)
    except Exception as e:
        print(f'Error downloading {checksum_url}: {e}')
        return

    # Check sha3-512sums of local copies against those in sn0.txt
    updated_files = {}
    downloaded_files = {}

    with open(Path(dest) / 'sn0.txt', 'r') as f:
        checksums = f.readlines()

    valid_files = set()
    for checksum in checksums:
        if checksum.startswith('#') or len(checksum.strip()) == 0:
            continue
        sha3_512sum, file_path = checksum.split(maxsplit=1)
        file_path = normalize_path(file_path.strip())
        valid_files.add(file_path)
        local_path = Path(dest) / file_path

        if file_path == 'index.html':
            continue  # Skip index.html in checksum verification

        if not local_path.exists():
            print(f'Downloading {file_path}...')
            download_url_to_file(urllib.parse.urljoin(source, file_path), source, dest, downloaded_files)
            updated_files[local_path] = sha3_512sum
        else:
            if os.path.isfile(local_path):
                with open(local_path, 'rb') as f:
                    content = f.read()
                    hash_object = hashlib.sha3_512(content)
                    hex_dig = hash_object.hexdigest()
                    if hex_dig == sha3_512sum:
                        print(f'{file_path} already up-to-date.')
                        downloaded_files[local_path] = sha3_512sum
                    else:
                        print(f'Downloading {file_path}...')
                        if bank:
                            backup_file(local_path)
                        download_url_to_file(urllib.parse.urljoin(source, file_path), source, dest, downloaded_files)
                        updated_files[local_path] = sha3_512sum
            else:
                print(f'Skipping directory: {file_path}')

    # Add content.json and index.html to valid_files
    valid_files.add('content.json')
    valid_files.add('index.html')

    # Melt subfolders
    if melt:
        for root, dirs, files in os.walk(dest):
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, dest)
                normalized_path = normalize_path(relative_path)
                if normalized_path not in valid_files and not file.endswith('.bak') and file not in ['sn0.txt', 'content.json', 'index.html']:
                    print(f'Removing {file_path}...')
                    os.remove(file_path)
        
        # Remove empty directories
        for root, dirs, files in os.walk(dest, topdown=False):
            for dir in dirs:
                dir_path = os.path.join(root, dir)
                if not os.listdir(dir_path):
                    print(f'Removing empty directory: {dir_path}')
                    os.rmdir(dir_path)

    # Download website
    try:
        html = download_url(source)
    except Exception as e:
        print(f'Error downloading {source}: {e}')
        return

    parser = LinkParser(source)
    parser.feed(html)
    urls = parser.links

    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        futures = []
        for url in urls:
            if source not in url:
                continue

            path = normalize_path(url.replace(source, ''))

            if path == '':
                path = 'index.html'

            dest_path = Path(dest) / path.lstrip('/')

            if os.path.isdir(dest_path):
                print(f'Skipping directory: {path}')
                continue

            if path != 'index.html':
                if dest_path in updated_files:
                    if updated_files[dest_path] == get_sha3_512sum(dest_path):
                        print(f'{path} already up-to-date.')
                        continue

                if dest_path in downloaded_files:
                    if downloaded_files[dest_path] == get_sha3_512sum(dest_path):
                        print(f'{path} already downloaded and verified.')
                        continue
                    else:
                        print(f'Re-downloading {path}...')
                        if bank:
                            backup_file(dest_path)
                else:
                    print(f'Downloading {path}...')
                
                if bank:
                    backup_file(dest_path)

            future = executor.submit(download_url_to_file, url, source, dest, downloaded_files)
            futures.append(future)

        for future in futures:
            try:
                result = future.result()
            except Exception as e:
                print(f'Error downloading {future}: {e}')

    # Always update index.html without backing up
    dest_path = Path(dest) / 'index.html'
    with open(dest_path, 'w') as f:
        f.write(html)
    print("index.html updated.")

    # Update links in index.html
    update_links_in_file(dest_path, urls, source)

def download_url_to_file(url, source, dest, downloaded_files):
    try:
        with urllib.request.urlopen(url) as response:
            content = response.read()
    except Exception as e:
        print(f'Error downloading {url}: {e}')
        return

    path = normalize_path(url.replace(source, ''))

    if path == '':
        path = 'index.html'

    dest_path = Path(dest) / path.lstrip('/')
    dir_path = dest_path.parent

    dir_path.mkdir(parents=True, exist_ok=True)

    if path != 'index.html':
        if dest_path in downloaded_files:
            if downloaded_files[dest_path] == get_sha3_512sum(dest_path):
                print(f'{path} already downloaded and verified.')
                return
            else:
                print(f'Re-downloading {path}...')

    try:
        with open(dest_path, 'wb') as f:
            f.write(content)
    except Exception as e:
        print(f'Error writing file {dest_path}: {e}')
        return

    if path != 'index.html':
        downloaded_files[dest_path] = get_sha3_512sum(dest_path)

def backup_file(file_path, content=None):
    if not os.path.isfile(file_path) or file_path.name == 'index.html':
        return  # Don't attempt to backup directories or index.html
    
    path, ext = os.path.splitext(file_path)
    i = 1
    original_path = f'{path}{ext}.{i}.bak'
    while os.path.exists(original_path):
        i += 1
        original_path = f'{path}{ext}.{i}.bak'
    if content is None:
        with open(file_path, 'rb') as f:
            content = f.read()
    with open(original_path, 'wb') as backup_file:
        backup_file.write(content)
    print(f'Backup created for {file_path} at {original_path}')

def download_url(url, binary=False):
    with urllib.request.urlopen(url) as response:
        if binary:
            return response.read()
        else:
            return response.read().decode('utf-8')

def update_links_in_file(file_path, links, base_url):
    print(f"Updating links in {file_path}")
    with open(file_path, 'r') as f:
        content = f.read()

    for link in links:
        print(f"Processing link: {link}")
        if link:  # Only process non-empty links
            try:
                relative_path = os.path.relpath(urllib.parse.urljoin(base_url, link), base_url)
                normalized_path = normalize_path(relative_path)
                if not normalized_path.startswith(('/', '..')):
                    normalized_path = './' + normalized_path
                # Use regex to replace the link, preserving quotes and other attributes
                content = re.sub(r'(href|src)=["\']' + re.escape(link) + r'["\']', 
                                 r'\1="' + normalized_path + '"', 
                                 content)
                print(f"  Replaced {link} with {normalized_path}")
            except ValueError as e:
                print(f"  Error processing {link}: {e}")
                # If relpath fails, use the link as is
                normalized_path = normalize_path(link)
                content = re.sub(r'(href|src)=["\']' + re.escape(link) + r'["\']', 
                                 r'\1="' + normalized_path + '"', 
                                 content)
                print(f"  Used normalized link instead: {normalized_path}")

    with open(file_path, 'w') as f:
        f.write(content)

def get_sha3_512sum(file_path):
    if not os.path.isfile(file_path):
        return None
    with open(file_path, 'rb') as f:
        content = f.read()
        hash_object = hashlib.sha3_512(content)
        hex_dig = hash_object.hexdigest()
        return hex_dig

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Download a website and its assets.')
    parser.add_argument('dest', help='The destination directory for the downloaded website.')
    parser.add_argument('source', help='The URL of the website to download.')
    parser.add_argument('--bank', action='store_true', help='Create a backup of files before overwriting them.')
    parser.add_argument('--melt', action='store_true', help='Delete local files in subfolders not listed in sn0.txt except for .bak files.')
    args = parser.parse_args()
    download_website(args.source, args.dest, args.bank, args.melt)