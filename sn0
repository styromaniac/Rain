#!/usr/bin/env python3

import os
import re
import urllib.parse
import urllib.request
import sys
from html.parser import HTMLParser
from concurrent.futures import ThreadPoolExecutor
import hashlib
from pathlib import Path
import argparse

def normalize_path(path):
    parts = path.split('/')
    normalized = []
    for part in parts:
        if part == '..':
            if normalized:
                normalized.pop()
        elif part and part != '.':
            normalized.append(part)
    return '/'.join(normalized)

class LinkParser(HTMLParser):
    def __init__(self, base_url):
        super().__init__()
        self.base_url = base_url
        self.links = []

    def handle_starttag(self, tag, attrs):
        if tag in ['a', 'link']:
            href = self.get_attr(attrs, 'href')
            if href is not None:
                self.add_link(href)
        elif tag in ['script', 'img', 'video', 'audio']:
            src = self.get_attr(attrs, 'src')
            if src is not None:
                self.add_link(src)

    def add_link(self, link):
        url = urllib.parse.urljoin(self.base_url, normalize_path(link))
        self.links.append(url)

    def get_attr(self, attrs, attr_name):
        for attr in attrs:
            if attr[0].lower() == attr_name:
                return attr[1]

def download_website(source, dest, bank, melt):
    # Download sn0.txt
    checksum_url = urllib.parse.urljoin(source, 'sn0.txt')
    try:
        with urllib.request.urlopen(checksum_url) as response:
            checksum_file = response.read()
        with open(Path(dest) / 'sn0.txt', 'wb') as f:
            f.write(checksum_file)
    except Exception as e:
        print(f'Error downloading {checksum_url}: {e}')
        return

    # Check sha3-512sums of local copies against those in sn0.txt
    updated_files = {}
    downloaded_files = {}

    with open(Path(dest) / 'sn0.txt', 'r') as f:
        checksums = f.readlines()

        for checksum in checksums:
            if checksum.startswith('#') or len(checksum.strip()) == 0:
                continue

            sha3_512sum, file_path = checksum.split(maxsplit=1)
            file_path = normalize_path(file_path.strip())
            local_path = Path(dest) / file_path

            if file_path == 'index.html':
                continue  # Skip index.html in checksum verification

            if not local_path.exists():
                print(f'Downloading {file_path}...')
                download_url_to_file(urllib.parse.urljoin(source, file_path), source, dest, downloaded_files)
                updated_files[local_path] = sha3_512sum
            else:
                if os.path.isfile(local_path):
                    with open(local_path, 'rb') as f:
                        content = f.read()
                        hash_object = hashlib.sha3_512(content)
                        hex_dig = hash_object.hexdigest()
                        if hex_dig == sha3_512sum:
                            print(f'{file_path} already up-to-date.')
                            downloaded_files[local_path] = sha3_512sum
                        else:
                            print(f'Downloading {file_path}...')
                            if bank:
                                backup_file(local_path)
                            download_url_to_file(urllib.parse.urljoin(source, file_path), source, dest, downloaded_files)
                            updated_files[local_path] = sha3_512sum
                else:
                    print(f'Skipping directory: {file_path}')

    # Melt subfolders
    if melt:
        subfolders = set([os.path.dirname(normalize_path(checksum.split(maxsplit=1)[1])) for checksum in checksums])
        for subfolder in subfolders:
            local_subfolder = Path(dest) / subfolder
            if not local_subfolder.exists():
                continue
            for file_path in local_subfolder.glob('*'):
                if file_path.name.endswith('.bak'):
                    continue
                if not any(str(file_path).endswith(normalize_path(checksum.split(maxsplit=1)[1].strip())) for checksum in checksums):
                    print(f'Removing {file_path}...')
                    os.remove(file_path)

    # Download website
    try:
        html = download_url(source)
    except Exception as e:
        print(f'Error downloading {source}: {e}')
        return

    parser = LinkParser(source)
    parser.feed(html)
    urls = parser.links

    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        futures = []
        for url in urls:
            if source not in url:
                continue

            path = normalize_path(url.replace(source, ''))

            if path == '':
                path = 'index.html'

            dest_path = Path(dest) / path.lstrip('/')

            if os.path.isdir(dest_path):
                print(f'Skipping directory: {path}')
                continue

            if path != 'index.html':
                if dest_path in updated_files:
                    if updated_files[dest_path] == get_sha3_512sum(dest_path):
                        print(f'{path} already up-to-date.')
                        continue

                if dest_path in downloaded_files:
                    if downloaded_files[dest_path] == get_sha3_512sum(dest_path):
                        print(f'{path} already downloaded and verified.')
                        continue
                    else:
                        print(f'Re-downloading {path}...')
                        if bank:
                            backup_file(dest_path)
                else:
                    print(f'Downloading {path}...')
                
                if bank:
                    backup_file(dest_path)

            future = executor.submit(download_url_to_file, url, source, dest, downloaded_files)
            futures.append(future)

        for future in futures:
            try:
                result = future.result()
            except Exception as e:
                print(f'Error downloading {future}: {e}')

    # Always update index.html without backing up
    dest_path = Path(dest) / 'index.html'
    with open(dest_path, 'w') as f:
        f.write(html)
    print("index.html updated.")

    # Update links in index.html
    index_links = [normalize_path(url.replace(source, '')) for url in urls if source in url and url.replace(source, '')]
    update_links_in_file(dest_path, index_links)

def download_url_to_file(url, source, dest, downloaded_files):
    try:
        with urllib.request.urlopen(url) as response:
            content = response.read()
    except Exception as e:
        print(f'Error downloading {url}: {e}')
        return

    path = normalize_path(url.replace(source, ''))

    if path == '':
        path = 'index.html'

    dest_path = Path(dest) / path.lstrip('/')
    dir_path = dest_path.parent

    dir_path.mkdir(parents=True, exist_ok=True)

    if path != 'index.html':
        if dest_path in downloaded_files:
            if downloaded_files[dest_path] == get_sha3_512sum(dest_path):
                print(f'{path} already downloaded and verified.')
                return
            else:
                print(f'Re-downloading {path}...')

    try:
        with open(dest_path, 'wb') as f:
            f.write(content)
    except Exception as e:
        print(f'Error writing file {dest_path}: {e}')
        return

    if path != 'index.html':
        downloaded_files[dest_path] = get_sha3_512sum(dest_path)

def backup_file(file_path, content=None):
    if not os.path.isfile(file_path) or file_path.name == 'index.html':
        return  # Don't attempt to backup directories or index.html
    
    path, ext = os.path.splitext(file_path)
    i = 1
    original_path = f'{path}{ext}.{i}.bak'
    while os.path.exists(original_path):
        i += 1
        original_path = f'{path}{ext}.{i}.bak'
    if content is None:
        with open(file_path, 'rb') as f:
            content = f.read()
    with open(original_path, 'wb') as backup_file:
        backup_file.write(content)
    print(f'Backup created for {file_path} at {original_path}')

def download_url(url, binary=False):
    with urllib.request.urlopen(url) as response:
        if binary:
            return response.read()
        else:
            return response.read().decode('utf-8')

def update_links_in_file(file_path, links):
    print(f"Updating links in {file_path}")
    with open(file_path, 'r') as f:
        content = f.read()

    for link in links:
        print(f"Processing link: {link}")
        if link:  # Only process non-empty links
            try:
                relative_path = os.path.relpath(link, os.path.dirname(file_path))
                normalized_path = normalize_path(relative_path)
                content = content.replace(link, normalized_path)
                print(f"  Replaced {link} with {normalized_path}")
            except ValueError as e:
                print(f"  Error processing {link}: {e}")
                # If relpath fails, use the link as is
                normalized_path = normalize_path(link)
                content = content.replace(link, normalized_path)
                print(f"  Used normalized link instead: {normalized_path}")

    with open(file_path, 'w') as f:
        f.write(content)

def get_sha3_512sum(file_path):
    if not os.path.isfile(file_path):
        return None
    with open(file_path, 'rb') as f:
        content = f.read()
        hash_object = hashlib.sha3_512(content)
        hex_dig = hash_object.hexdigest()
        return hex_dig

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Download a website and its assets.')
    parser.add_argument('dest', help='The destination directory for the downloaded website.')
    parser.add_argument('source', help='The URL of the website to download.')
    parser.add_argument('--bank', action='store_true', help='Create a backup of files before overwriting them.')
    parser.add_argument('--melt', action='store_true', help='Delete local files in subfolders not listed in sn0.txt except for .bak files.')
    args = parser.parse_args()
    download_website(args.source, args.dest, args.bank, args.melt)