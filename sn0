#!/usr/bin/env python3

import os
import re
import urllib.parse
import urllib.request
import sys
from html.parser import HTMLParser
from concurrent.futures import ThreadPoolExecutor
import hashlib
from pathlib import Path
import argparse
import time
from email.utils import parsedate_to_datetime, formatdate

def normalize_path(path):
    # Preserve the leading slash if it exists
    leading_slash = '/' if path.startswith('/') else ''
    
    # Split the path into parts
    parts = path.split('/')
    # Initialize a stack to keep track of the directory levels
    stack = []
    for part in parts:
        if part == '.' or part == '':
            continue
        elif part == '..':
            if stack and stack[-1] != '..':
                stack.pop()
            else:
                stack.append(part)
        else:
            stack.append(part)
    # Join the parts back together, preserving the leading slash
    return leading_slash + '/'.join(stack)

class LinkParser(HTMLParser):
    def __init__(self, base_url):
        super().__init__()
        self.base_url = base_url
        self.links = []
        self.current_tag = None

    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        if tag in ['a', 'link']:
            href = self.get_attr(attrs, 'href')
            if href is not None:
                self.add_link(href)
        elif tag in ['script', 'img', 'video', 'audio']:
            src = self.get_attr(attrs, 'src')
            if src is not None:
                self.add_link(src)

    def handle_endtag(self, tag):
        self.current_tag = None

    def handle_data(self, data):
        if self.current_tag == 'script':
            # Look for URLs in inline JavaScript
            urls = re.findall(r'(?:(?:https?|ftp):\/\/)?[\w/\-?=%.]+\.[\w/\-?=%.]+', data)
            for url in urls:
                self.add_link(url)

    def add_link(self, link):
        url = urllib.parse.urljoin(self.base_url, link)
        if url not in self.links:
            self.links.append(url)

    def get_attr(self, attrs, attr_name):
        for attr in attrs:
            if attr[0].lower() == attr_name:
                return attr[1]

def download_website(source, dest, bank, melt):
    # Download sn0.txt using conditional GET
    if not download_sn0_txt(source, dest):
        print('sn0.txt is up-to-date.')

    # Check sha3-512sums of local copies against those in sn0.txt
    updated_files = {}
    downloaded_files = {}

    sn0_path = Path(dest) / 'sn0.txt'
    if not sn0_path.exists():
        print('sn0.txt not found. Exiting.')
        return

    with open(sn0_path, 'r') as f:
        checksums = f.readlines()

    valid_files = set()
    for checksum in checksums:
        if checksum.startswith('#') or len(checksum.strip()) == 0:
            continue
        sha3_512sum, file_path = checksum.split(maxsplit=1)
        file_path = normalize_path(file_path.strip())
        valid_files.add(file_path)
        local_path = Path(dest) / file_path

        if file_path == 'index.html':
            continue  # Skip index.html in checksum verification

        dest_path = Path(dest) / file_path.lstrip('/')

        if not dest_path.exists():
            print(f'Downloading {file_path}...')
            download_file_conditional(urllib.parse.urljoin(source, file_path), dest_path, bank)
            updated_files[dest_path] = sha3_512sum
        else:
            if dest_path.is_file():
                local_checksum = get_sha3_512sum(dest_path)
                if local_checksum == sha3_512sum:
                    print(f'{file_path} already up-to-date.')
                    downloaded_files[dest_path] = sha3_512sum
                else:
                    print(f'Downloading {file_path}...')
                    if bank:
                        backup_file(dest_path)
                    download_file_conditional(urllib.parse.urljoin(source, file_path), dest_path, bank)
                    updated_files[dest_path] = sha3_512sum
            else:
                print(f'Skipping directory: {file_path}')

    # Add content.json and index.html to valid_files
    valid_files.add('content.json')
    valid_files.add('index.html')

    # Melt subfolders
    if melt:
        for root, dirs, files in os.walk(dest):
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, dest)
                normalized_path = normalize_path(relative_path)
                if normalized_path not in valid_files and not file.endswith('.bak') and file not in ['sn0.txt', 'content.json', 'index.html']:
                    print(f'Removing {file_path}...')
                    os.remove(file_path)
        
        # Remove empty directories
        for root, dirs, files in os.walk(dest, topdown=False):
            for dir in dirs:
                dir_path = os.path.join(root, dir)
                if not os.listdir(dir_path):
                    print(f'Removing empty directory: {dir_path}')
                    os.rmdir(dir_path)

    # Download website
    try:
        html = download_url_conditional(source, Path(dest) / 'index.html', bank)
    except Exception as e:
        print(f'Error downloading {source}: {e}')
        return

    print("index.html updated.")

    parser = LinkParser(source)
    parser.feed(html)
    urls = parser.links

    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        futures = []
        for url in urls:
            if source not in url:
                continue

            path = normalize_path(url.replace(source, ''))

            if path == '':
                continue  # Skip index.html as it's already been updated

            dest_path = Path(dest) / path.lstrip('/')

            if dest_path.is_dir():
                print(f'Skipping directory: {path}')
                continue

            future = executor.submit(download_file_conditional, url, dest_path, bank)
            futures.append(future)

        for future in futures:
            try:
                future.result()
            except Exception as e:
                print(f'Error downloading file: {e}')

    # Update links in index.html
    update_links_in_file(Path(dest) / 'index.html', urls, source)

def download_sn0_txt(source, dest):
    import urllib.error
    sn0_path = Path(dest) / 'sn0.txt'
    headers = {}
    if sn0_path.exists():
        last_modified_time = os.path.getmtime(sn0_path)
        last_modified_str = formatdate(timeval=last_modified_time, usegmt=True)
        headers['If-Modified-Since'] = last_modified_str

    checksum_url = urllib.parse.urljoin(source, 'sn0.txt')
    req = urllib.request.Request(checksum_url, headers=headers)
    try:
        with urllib.request.urlopen(req) as response:
            if response.status == 304:
                print('sn0.txt is up-to-date.')
                return False
            else:
                checksum_file = response.read()
                with open(sn0_path, 'wb') as f:
                    f.write(checksum_file)
                # Update the file's modification time to match the server's
                if 'Last-Modified' in response.headers:
                    server_time = parsedate_to_datetime(response.headers['Last-Modified'])
                    os.utime(sn0_path, (server_time.timestamp(), server_time.timestamp()))
                print('sn0.txt downloaded and updated.')
                return True
    except urllib.error.HTTPError as e:
        if e.code == 304:
            print('sn0.txt is up-to-date.')
            return False
        else:
            print(f'Error downloading sn0.txt: {e}')
            return False
    except Exception as e:
        print(f'Error downloading sn0.txt: {e}')
        return False

def download_file_conditional(url, dest_path, bank):
    headers = {}
    if dest_path.exists():
        last_modified_time = os.path.getmtime(dest_path)
        last_modified_str = formatdate(timeval=last_modified_time, usegmt=True)
        headers['If-Modified-Since'] = last_modified_str

    req = urllib.request.Request(url, headers=headers)
    try:
        with urllib.request.urlopen(req) as response:
            if response.status == 304:
                print(f'{dest_path} is up-to-date.')
                return
            else:
                content = response.read()
                dir_path = dest_path.parent
                dir_path.mkdir(parents=True, exist_ok=True)
                if bank and dest_path.exists():
                    backup_file(dest_path)
                with open(dest_path, 'wb') as f:
                    f.write(content)
                # Update the file's modification time to match the server's
                if 'Last-Modified' in response.headers:
                    server_time = parsedate_to_datetime(response.headers['Last-Modified'])
                    os.utime(dest_path, (server_time.timestamp(), server_time.timestamp()))
                print(f'{dest_path} downloaded and updated.')
    except urllib.error.HTTPError as e:
        if e.code == 304:
            print(f'{dest_path} is up-to-date.')
        else:
            print(f'Error downloading {url}: {e}')
    except Exception as e:
        print(f'Error downloading {url}: {e}')

def download_url_conditional(url, dest_path, bank):
    headers = {}
    if dest_path.exists():
        last_modified_time = os.path.getmtime(dest_path)
        last_modified_str = formatdate(timeval=last_modified_time, usegmt=True)
        headers['If-Modified-Since'] = last_modified_str

    req = urllib.request.Request(url, headers=headers)
    try:
        with urllib.request.urlopen(req) as response:
            if response.status == 304 and dest_path.exists():
                print(f'{dest_path} is up-to-date.')
                with open(dest_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                content = response.read().decode('utf-8')
                dir_path = dest_path.parent
                dir_path.mkdir(parents=True, exist_ok=True)
                if bank and dest_path.exists():
                    backup_file(dest_path)
                with open(dest_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                # Update the file's modification time to match the server's
                if 'Last-Modified' in response.headers:
                    server_time = parsedate_to_datetime(response.headers['Last-Modified'])
                    os.utime(dest_path, (server_time.timestamp(), server_time.timestamp()))
                print(f'{dest_path} downloaded and updated.')
                return content
    except urllib.error.HTTPError as e:
        if e.code == 304 and dest_path.exists():
            print(f'{dest_path} is up-to-date.')
            with open(dest_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            print(f'Error downloading {url}: {e}')
            raise
    except Exception as e:
        print(f'Error downloading {url}: {e}')
        raise

def backup_file(file_path):
    if not os.path.isfile(file_path) or file_path.name == 'index.html':
        return  # Don't attempt to backup directories or index.html
    
    path, ext = os.path.splitext(str(file_path))
    i = 1
    backup_path = f'{path}.{i}{ext}.bak'
    while os.path.exists(backup_path):
        i += 1
        backup_path = f'{path}.{i}{ext}.bak'
    with open(file_path, 'rb') as f_in, open(backup_path, 'wb') as f_out:
        f_out.write(f_in.read())
    print(f'Backup created for {file_path} at {backup_path}')

def update_links_in_file(file_path, links, base_url):
    print(f"Updating links in {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    for link in links:
        if link:  # Only process non-empty links
            try:
                relative_path = urllib.parse.urlparse(link).path
                normalized_path = normalize_path(relative_path)
                if not normalized_path.startswith(('/', '..')):
                    normalized_path = './' + normalized_path
                # Use regex to replace the link, preserving quotes and other attributes
                pattern = r'(?P<prefix>(?:href|src)=["\'])' + re.escape(link) + r'(?P<suffix>["\'])'
                replacement = r'\g<prefix>' + normalized_path + r'\g<suffix>'
                content, count = re.subn(pattern, replacement, content)
                if count > 0:
                    print(f"  Replaced {link} with {normalized_path}")
            except ValueError as e:
                print(f"  Error processing {link}: {e}")
                # If relpath fails, use the link as is
                normalized_path = normalize_path(link)
                pattern = r'(?P<prefix>(?:href|src)=["\'])' + re.escape(link) + r'(?P<suffix>["\'])'
                replacement = r'\g<prefix>' + normalized_path + r'\g<suffix>'
                content = re.sub(pattern, replacement, content)
                print(f"  Used normalized link instead: {normalized_path}")

    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

def get_sha3_512sum(file_path):
    if not os.path.isfile(file_path):
        return None
    with open(file_path, 'rb') as f:
        content = f.read()
        hash_object = hashlib.sha3_512(content)
        hex_dig = hash_object.hexdigest()
        return hex_dig

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Download a website and its assets.')
    parser.add_argument('dest', help='The destination directory for the downloaded website.')
    parser.add_argument('source', help='The URL of the website to download.')
    parser.add_argument('--bank', action='store_true', help='Create a backup of files before overwriting them.')
    parser.add_argument('--melt', action='store_true', help='Delete local files in subfolders not listed in sn0.txt except for .bak files.')
    args = parser.parse_args()
    download_website(args.source, args.dest, args.bank, args.melt)